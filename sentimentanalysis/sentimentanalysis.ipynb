{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x98 in position 1071: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mUnicodeDecodeError\u001B[0m                        Traceback (most recent call last)",
      "\u001B[1;32mC:\\Users\\MAXIM~1.KOR\\AppData\\Local\\Temp/ipykernel_12224/418075671.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[1;32mwith\u001B[0m \u001B[0mopen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'movie_data.csv'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'w'\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mf\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      6\u001B[0m     \u001B[1;31m# f.writelines([l.decode('utf-8') for l in text])\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 7\u001B[1;33m     \u001B[0mf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mwritelines\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0ml\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdecode\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'cp1251'\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0ml\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mtext\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32mC:\\Users\\MAXIM~1.KOR\\AppData\\Local\\Temp/ipykernel_12224/418075671.py\u001B[0m in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[1;32mwith\u001B[0m \u001B[0mopen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'movie_data.csv'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'w'\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mf\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      6\u001B[0m     \u001B[1;31m# f.writelines([l.decode('utf-8') for l in text])\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 7\u001B[1;33m     \u001B[0mf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mwritelines\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0ml\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdecode\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'cp1251'\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0ml\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mtext\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32mc:\\users\\maxim.korolev\\appdata\\local\\programs\\python\\python39\\lib\\encodings\\cp1251.py\u001B[0m in \u001B[0;36mdecode\u001B[1;34m(self, input, errors)\u001B[0m\n\u001B[0;32m     13\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     14\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mdecode\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0merrors\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'strict'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 15\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mcodecs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcharmap_decode\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0merrors\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mdecoding_table\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     16\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     17\u001B[0m \u001B[1;32mclass\u001B[0m \u001B[0mIncrementalEncoder\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcodecs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mIncrementalEncoder\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mUnicodeDecodeError\u001B[0m: 'charmap' codec can't decode byte 0x98 in position 1071: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "# import gzip\n",
    "#\n",
    "# with gzip.open('movie_data.csv.gz', 'r') as gz:\n",
    "#     text = gz.readlines()\n",
    "# with open('movie_data.csv', 'w') as f:\n",
    "#     f.writelines([l.decode('utf-8') for l in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('movie_data.csv')\n",
    "df.columns = ['review', 'sentiment']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "df.to_csv('movie_data.csv', index=False, encoding='utf-8')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "                                              review  sentiment\n0  Election is a Chinese mob movie, or triads in ...          1\n1  I was just watching a Forensic Files marathon ...          0\n2  Police Story is a stunning series of set piece...          1",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Election is a Chinese mob movie, or triads in ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>I was just watching a Forensic Files marathon ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Police Story is a stunning series of set piece...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('movie_data.csv', encoding='utf-8')\n",
    "df.head(3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "(50000, 2)"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 6, 'sun': 4, 'is': 1, 'shining': 3, 'weather': 8, 'sweet': 5, 'and': 0, 'one': 2, 'two': 7}\n",
      "[[0 1 0 1 1 0 1 0 0]\n",
      " [0 1 0 0 0 1 1 0 1]\n",
      " [2 3 2 1 1 1 2 1 1]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count = CountVectorizer()\n",
    "docs = np.array(['The sun is shining',\n",
    "                 'The weather is sweet',\n",
    "                 'The sun is shining, the weather is sweet, '\n",
    "                 'and one and one is two'])\n",
    "bag = count.fit_transform(docs)\n",
    "print(count.vocabulary_)\n",
    "print(bag.toarray())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.   0.43 0.   0.56 0.56 0.   0.43 0.   0.  ]\n",
      " [0.   0.43 0.   0.   0.   0.56 0.43 0.   0.56]\n",
      " [0.5  0.45 0.5  0.19 0.19 0.19 0.3  0.25 0.19]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf = TfidfTransformer(use_idf=True, norm='l2', smooth_idf=True)\n",
    "np.set_printoptions(precision=2)\n",
    "print(tfidf.fit_transform(count.fit_transform(docs)).toarray())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "'nd three more acting performances (including Yam).'"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[0, 'review'][-50:]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "'nd three more acting performances including yam '"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from commons import preprocessor\n",
    "\n",
    "preprocessor(df.loc[0, 'review'][-50:])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "'this is Ð° test : ('"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor(\"</a>This :) is : ( Ð° test :-) !\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "df['review'] = df['review'].apply(preprocessor)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "['runners', 'like', 'running', 'and', 'thus', 'they', 'run']"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from commons import tokenizer\n",
    "\n",
    "tokenizer('runners like running and thus they run')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "['runner', 'like', 'run', 'and', 'thu', 'they', 'run']"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "\n",
    "\n",
    "def tokenizer_porter(text):\n",
    "    return [porter.stem(word) for word in text.split()]\n",
    "\n",
    "\n",
    "tokenizer_porter('runners like running and thus they run')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\maxim.korolev\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "['runner', 'like', 'run', 'run', 'lot']"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "[w for w in tokenizer_porter('a runner likes running and runs a lot')[-10:]\n",
    " if w not in stop]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "X_train = df.loc[:25000, 'review'].values\n",
    "y_train = df.loc[:25000, 'sentiment'].values\n",
    "X_test = df.loc[25000:, 'review'].values\n",
    "y_test = df.loc[25000:, 'sentiment'].values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(strip_accents=None, lowercase=False, preprocessor=None)\n",
    "param_grid = [{'vect__ngram_range': [(1, 1)],\n",
    "               'vect__stop_words': [stop, None],\n",
    "               'vect__tokenizer': [tokenizer, tokenizer_porter],\n",
    "               'clf__penalty': ['l1', 'l2'],\n",
    "               'clf__C': [1.0, 10.0, 100.0]},\n",
    "              {'vect__ngram_range': [(1, 1)],\n",
    "               'vect__stop_words': [stop, None],\n",
    "               'vect__tokenizer': [tokenizer, tokenizer_porter],\n",
    "               'vect__use_idf': [False],\n",
    "               'vect__norm': [None],\n",
    "               'clf__penalty': ['l1', 'l2'],\n",
    "               'clf__C': [1.0, 10.0, 100.0]}]\n",
    "lr_tfidf = Pipeline([('vect', tfidf),\n",
    "                     ('clf',\n",
    "                      LogisticRegression(random_state=1,\n",
    "                                         solver='liblinear'))])\n",
    "gs_lr_tfidf = GridSearchCV(lr_tfidf, param_grid, scoring='accuracy', cv=5, verbose=2, n_jobs=-1)\n",
    "gs_lr_tfidf.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}